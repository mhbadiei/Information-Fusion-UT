# Information Fusion Exercises Repository

Welcome to the Information Fusion Exercises Repository! This repository contains a series of exercises related to information fusion, covering various topics and techniques. Whether you are a student or a professional, these exercises will help you gain insights into data estimation, fusion methods, classification, and more.  

## Overview

This repository is organized into multiple exercises, each focusing on different aspects of information fusion. Below, you'll find an overview of the exercises and their key objectives:

### Exercise 1: Data Estimation and Fusion

In this exercise, you will explore methods related to data estimation and fusion. The exercise comprises explanatory questions and computational tasks that involve simulating data, calculating error metrics, obtaining OWA operator weights, formulating a learning method, and comparing multiple estimation methods.

### Exercise 2: Bayesian Theory and Dempster-Shafer Theory

Exercise 2 delves into Bayesian Theory and Dempster-Shafer Theory. It involves combining sensor data using Bayesian theory, pairwise data combination, determining fault scenarios, and applying Dempster-Shafer combination rules to analyze information fusion.

### Exercise 3: Data Combination using Sugeno and Choquet Integrals

Exercise 3 focuses on data combination using Sugeno and Choquet Integrals. It involves tasks related to arranging preferences, implementing fusion methods programmatically, and comparing results obtained through manual calculations and code.

### Exercise 4: Nonlinear State Estimation

In Exercise 4, you'll perform a comparison between nonlinear state estimation methods using the Inverted Pendulum-Cart system as a case study. The exercise includes steps to discretize a continuous-time system, estimate states using Particle Filter (PF), Unscented Kalman Filter (UKF), and Extended Kalman Filter (EKF), and evaluate the estimation results.

### Exercise 5: Multi-Classifier Classification System

Exercise 5 involves designing and implementing a multi-classifier classification system. It includes tasks related to generating data sets, splitting data, evaluating individual classifiers, conducting repeated experiments, fusing classifier outputs, and analyzing the impact of feature space dimensionality on classification performance.

### Project: Utilizing Data Fusion Techniques in a Multi-Classifier System for Natural Language Processing on Big Data within the Hadoop Framework.
The project's core focus is on text classification and leveraging distributed data processing techniques offered by Hadoop. It explores different fusion methods to combine the outputs of various models. The results demonstrate the effectiveness of these approaches in handling big data and natural language processing tasks.

## How to Use

1. Clone this repository to your local machine using `git clone`.

2. Navigate to the exercise folder you want to work on (e.g., `Exercise1`, `Exercise2`, etc.).

3. Read the exercise instructions provided in each folder's README.md file.

4. Implement your solutions and code examples based on the instructions.

5. Use the provided resources and code samples to enhance your understanding of information fusion concepts.

6. For any questions or clarifications, feel free to create an issue in this repository.

### Exercise 1: Data Estimation and Fusion

#### Explanatory Questions
- **The Uncertain OWA Operator (Part A):** Describe the Uncertain OWA Operator method, including its formulation, and highlight any advantages or disadvantages.

- **Induced OWA Operators (Part B):** Explain Induced OWA Operators, providing a formulation, and discussing any pros and cons.

- **Linguistic OWA Operators (Part C):** Elaborate on Linguistic OWA Operators, including their formulation, and mention any advantages or disadvantages if applicable.

- **Maximum Bayesian Entropy for obtaining weights (Part D):** Detail the Maximum Bayesian Entropy method for weight determination.

#### Computational Questions
Conduct simulations using Python or MATLAB based on a provided dataset. The dataset consists of four columns, with the fourth column representing real reference data. The first three columns contain noisy measurements generated by the Mackey-Glass model. The goal is to estimate the real data using various operators. Calculate the Mean Absolute Error (MAE), Root Mean Square Error (RMSE), and Mean Squared Error (MSE) for the estimated values compared to the real values, and report these error values for all nine scenarios.

#### Obtaining OWA Operator Weights
Explain the process of obtaining OWA operator weights using the algorithm described in the second part of the article. Calculate the estimated values using these weights and determine the Orness and Dispersion. Plot the estimated values alongside the real values, and calculate and compare the MAE, RMSE, and MSE.

#### Learning Method
Formulate a learning method using the provided data. Calculate the mean and variance of the data vector, introduce it into a Gaussian function, and plot it. Additionally, separately plot the Gaussian function for the error vector and analyze these plots.

#### Additional Methods
Choose four other methods for estimating the real values. Besides the methods themselves, report results in the form of pessimistic and optimistic scenarios. Present the results in tables and figures, including comparisons of errors, and explain if any error reduction has occurred.

### Exercise 1: Data Estimation and Fusion

**Explanatory Questions:**
Explain each of the following methods and provide a formulation for them, including any advantages and disadvantages if applicable:
- The Uncertain OWA Operator (Part A)
- Induced OWA Operators (Part B)
- Linguistic OWA Operators (Part C)
- Maximum Bayesian Entropy for obtaining weights (Part D)

**Computational Questions:**
Conduct simulations using Python or MATLAB based on a provided dataset. The dataset consists of four columns. The fourth column represents real reference data. The first three columns contain noisy measurements generated by the Mackey-Glass model. The goal is to estimate the real data using various operators. Calculate the Mean Absolute Error (MAE), Root Mean Square Error (RMSE), and Mean Squared Error (MSE) for the estimated values compared to the real values. Report these error values for all nine scenarios.

**Obtaining OWA Operator Weights:**
Explain how to obtain OWA operator weights using the algorithm described in the second part of the article. Then, calculate the estimated values using these weights and determine the Orness and Dispersion. Plot the estimated values alongside the real values. Calculate and compare the MAE, RMSE, and MSE.

**Learning Method:**
Formulate a learning method using the provided data. Calculate the mean and variance of the data vector, introduce it into a Gaussian function, and plot it. Additionally, separately plot the Gaussian function for the error vector. Analyze these plots.

**Additional Methods:**
Choose four other methods for estimating the real values. Besides the methods themselves, report results in the form of pessimistic and optimistic scenarios. Present the results in tables and figures, including comparisons of errors. Explain if any error reduction has occurred.

### Exercise 2: Bayesian Theory and Dempster-Shafer Theory

**Part A: Bayesian Theory**

**a) Combining Sensor Data:**
Combine the data from three sensors using Bayesian theory, taking into account the data in the table and the results shown from the sensors. Report the result concerning the gas toxicity levels.

**b) Pairwise Data Combination:**
Instead of combining data from all three sensors, combine the data pairwise and compare the results with the approach in part (a). Explain your observations.

**Part B: Dempster-Shafer Theory**

**a) Fault Scenarios:**
Determine all possible states in which a fault may occur.

**Operators and Their Observations:**
Suppose two operators named Operator 1 and Operator 2 are inspecting for faults in the power plant. Operator 1 is an expert in fault detection, and Operator 2 observes different scenarios for the same faults. Based on their experience, both operators provide the following mass functions:

**For Operator 1:**
(Insert the provided mass functions for Operator 1 here)

**For Operator 2:**
(Insert the provided mass functions for Operator 2 here)

**b) Plausibility and Belief:**
Complete the plausibility and belief tables based on the provided information.

**c) Reducing the Table:**
Due to the weight of some events being zero according to each operator, remove the corresponding rows and columns from the table for computational efficiency. Calculate the reduced table.

**d) Dempster-Shafer Combination:**
Using Dempster-Shafer combination rules, calculate the weights for each event in the reduced table.

**e) Importance of Events:**
Discuss which events in the reduced table should be given more importance based on plausibility and belief values. If you had a Bayesian perspective, which fault condition should be more considered? Compare the performance of Bayesian and Dempster-Shafer combination for information fusion.

### Exercise 3: Data Combination using Sugeno and Choquet Integrals

#### Part A: Data Combination

**a) Ordering Films by Preferences:**
Arrange the films according to the preferences of individuals as follows:

Sugeno/Choquet: F1 < F2 < F3 < F4

Calculate the order for part (a) and provide your answer.

**b) Two Individuals:**
Now, consider only two individuals and repeat part (a).

**c) Two Different Individuals:**
Consider only two individuals different from those in part (b) and repeat part (a).

**d) Two Other Individuals:**
Consider only two different individuals from the ones used in part (c) and repeat part (a).

#### Part B: Implementation

In this part, we aim to perform the tasks from parts a, b, c, and d of the previous section using programming and creating a function in MATLAB or Python. For this purpose, consider the following:

1. Write functions for Sugeno and Choquet integrals that take the weights (mu) and scores (f) as inputs. For example, the function should be called like this:
ans = Sugeno(mu, f)
The function you write should adapt itself based on the number of inputs, calculate the necessary weights for valid combinations, and perform all required calculations accurately. For example, it should return the correct output (the same as you manually calculated) for the following two expressions:
python
Copy code
Choquet Fusion of Scores for Movie A = Choquet([0.3, 0.6, 0.7, 0.3], [1.0, 0.8, 0.1, 0.8])
Choquet Fusion of Scores for Movie A = Choquet([0.3, 0.6], [1.0, 0.8])
Present all results obtained from the written functions alongside the manually calculated values in a table format. Include this table in your report.

### Exercise 4: Nonlinear State Estimation

In this exercise, we aim to make a comparison between nonlinear state estimation methods. We consider the well-known Inverted Pendulum-Cart system.

The state transition equations for this system are given as follows:

x˙ 1 = x2
x˙ 2 = u cos(x1) − (M + m)g sin(x1) + ml(cos(x1) sin(x1))x22 / (ml cos2(x1) − (M + m)l)
x˙ 3 = x4
x˙ 4 = u + ml(sin(x1))x22 / (M + m − mcos2(x1))


The output (measurement) equations for this system are as follows:
y1 = x1
y2 = x3


Consider the initial conditions for states and inputs to be zero. The parameter values are as follows:
m = 0.23
M = 2.40
g = 9.81
l = 0.36


**Step 1:**

The given continuous-time system needs to be discretized. Perform this discretization considering a sampling time of 1 millisecond. The simulation duration should be 10 seconds.

**Step 2:**

Consider process and measurement noises as Gaussian with zero mean and the following covariance matrices:
Q =
[0.9 0 0 0
0 0.8 0 0
0 0 0.9 0
0 0 0 0.8]

R =
[1.2 0
0 0.8]


Estimate all 4 states for each state using Particle Filter (PF), Unscented Kalman Filter (UKF), and Extended Kalman Filter (EKF).

**Step 3:**

For each of the real estimation methods, show the results for 3 estimates separately (show 4 signals in each figure). You are free to choose and weight the signals.

**Step 4:**

In this step, you need to calculate the Root Mean Squared Error (RMSE) for all 4 system states and for each of the 3 estimation methods. Perform this calculation for 100 Monte Carlo simulations and show the error trends in a figure for each system state. In your code, make sure not to use `rng` functions to ensure consistent results. The input to the system should be from a uniform distribution U[-0.2, 0.2] in each simulation.

Important notes:
- The system input should be uniformly distributed in the range [-0.2, 0.2] for each simulation.
- Before running the simulations, generate your input in a separate file and save it in an Excel file so that you can use it in MATLAB or Python.
- Note that the simulation time starts from second 0, and it ends at second 10. Since the sampling time is 1 millisecond, you will have 10,001 time steps. In MATLAB, you will have k = 1, 2, ..., 2000, 2001, ..., 10001, and in Python, you will have k = 0, 1, ..., 2000, 2001, ..., 10000.
- Also, keep in mind that, as mentioned before, the initial conditions are zero, and you should consider the first entry as zero and obtain the rest from a uniform distribution.

### Exercise 5: Multi-Classifier Classification System

#### Description
Design and implement a multi-classifier classification system employing five classifiers:
- Decision tree with selected parameters
- SVM with chosen kernel and parameters
- KNN with two selected K values
- Naïve Bayes

#### Tasks

**a) Generate Data Sets**
- Create two data sets, X and Y, each with 500 data points in a 100-dimensional feature space.
- Both classes are equiprobable, following Gaussian distributions with arbitrary means m1 and m2, with a variable Euclidean distance (d) between them.
- Covariance matrices are SX = 0.2I and SY = 0.4I, where I is the 100 × 100 identity matrix.

**b) Data Splitting**
- Select 70% of data in X1 and Y1 as a training set.
- Use the remaining 30% in X2 and Y2 as a test set.

**c) Individual Classifier Evaluation**
- Evaluate the accuracy and precision of each individual classifier on the test data concerning the distance between class means (d).
- Adjust classifier parameters for optimal performance.
- Plot accuracy vs. d and precision vs. d curves for each classifier.

**d) Repeated Experiment**
- Repeat parts b and c 100 times with random data splits.
- Calculate the average accuracy curve and its standard deviation.

**e) Classifier Fusion**
- Repeat parts b, c, and d but this time, combine classifier outputs in a five-classifier fusion system.
- Use simple majority voting, decision template, and OWA to combine results.
- Discuss the results and the fusion system's effect on classification performance compared to individual classifiers.

**f) Dimensionality Analysis**
- Investigate the role of feature space dimensionality in the fusion system's performance.
- Change dimensionality from 1 to 100 while fixing d for the best individual classifier test accuracy at 70%.
- Plot the accuracy vs. dimensionality for the multi-classifier system and provide insights.

#### Comments
- You can implement code in Matlab, Python, R, or any other programming language.
- Use scikit-learn package if desired.
- An example: In Matlab, consider a 2-dimensional space with two equiprobable classes, Gaussian distributions with means m1 = [0, 0]T and m2 = [1.2, 1.2]T, and covariance matrices S1 = S2 = 0.2I.

### Project: Utilizing Data Fusion Techniques in a Multi-Classifier System for Natural Language Processing on Big Data within the Hadoop Framework
This project focuses on the application of Natural Language Processing (NLP) techniques to handle Big Data. It explores various data fusion approaches in the context of a Multi-Classifier Fusion System based on the Hadoop framework. The main objectives and components of the project include:

#### N-grams Model: 
Implementing and studying N-grams, which are statistical models that predict the next word in a text based on the previous words.

#### Naïve Bayes Algorithm: 
Investigating the role of the Naïve Bayes algorithm in natural language processing for classification tasks.

#### Hadoop Integration: 
Utilizing the Hadoop Distributed File System (HDFS) and MapReduce algorithm to efficiently process and analyze large volumes of textual data.

#### Multi-Classifier Fusion: 
Creating a multi-classifier fusion system that combines the results of different models, including N-grams and Naïve Bayes, to improve accuracy.

#### Poet Recognition: 
Developing a system capable of recognizing the poet based on a given stanza of poetry. The system achieves a maximum accuracy of approximately 82.5% by analyzing the words in the stanza.



